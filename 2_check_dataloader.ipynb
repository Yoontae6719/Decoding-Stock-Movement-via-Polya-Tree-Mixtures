{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0493333-50e3-4d38-adc9-a11395678495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, PowerTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Dataset_SNP(Dataset):\n",
    "    def __init__(self, args, root_path, flag='train', data_path='SNP.csv', scale=True, stop_loss = 0):\n",
    "        \n",
    "        self.args = args\n",
    "\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.scale = scale\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.stop_loss = stop_loss\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        # Step 1. Get dataset\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
    "        df_raw['Date'] = pd.to_datetime(df_raw['Date'])\n",
    "\n",
    "        df_raw = df_raw.drop([\"Stock\"], axis=1)\n",
    "        df_raw = df_raw.dropna()\n",
    "\n",
    "        df_raw[['Y', 'Y_2', 'Y_3', 'Y_4']] = df_raw[['Y', 'Y_2', 'Y_3', 'Y_4']].apply(lambda x: x.map({'SELL': 0, 'BUY': 1}))\n",
    "        \n",
    "        # Step 2. Train // valid // test\n",
    "        num_train = df_raw[(df_raw['Date'] >= '2020-01-01') & (df_raw['Date'] <= '2022-12-31')].shape[0]\n",
    "        num_vali =  df_raw[(df_raw['Date'] >= '2023-01-01') & (df_raw['Date'] <= '2023-12-31')].shape[0]\n",
    "        num_test =  df_raw[(df_raw['Date'] >= '2024-01-01')].shape[0] \n",
    "        \n",
    "        border1s = [0,\n",
    "                    num_train,\n",
    "                    len(df_raw) - num_test]\n",
    "        \n",
    "        border2s = [num_train,\n",
    "                    num_train + num_vali,\n",
    "                    len(df_raw)]\n",
    "        \n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        # Step 3. Scaling\n",
    "        df_x = df_raw.drop(['Y', 'Y_2', 'Y_3', 'Y_4','Y_5'], axis=1).drop([\"Date\"], axis=1)\n",
    "        if self.scale:\n",
    "            train_data = df_x[border1s[0]:border2s[0]]\n",
    "            quantile_train = np.copy(train_data.values).astype(np.float64)\n",
    "            stds = np.std(quantile_train, axis=0, keepdims=True)\n",
    "            noise_std = 1e-3 / np.maximum(stds, 1e-3)\n",
    "            quantile_train += noise_std * np.random.randn(*quantile_train.shape)\n",
    "            self.scaler = QuantileTransformer(output_distribution='normal', random_state=1004)\n",
    "            self.scaler.fit(quantile_train)  \n",
    "            data_all = self.scaler.transform(df_x.values)  \n",
    "        else:\n",
    "            data_all = df_x.values\n",
    "\n",
    "\n",
    "        self.data_x = data_all[border1:border2]  \n",
    "        if self.stop_loss == 0:\n",
    "            df_y = df_raw[['Y']].values\n",
    "            self.data_y = df_y[border1:border2]\n",
    "        elif self.stop_loss == 2:\n",
    "            df_y = df_raw[['Y_2']].values\n",
    "            self.data_y = df_y[border1:border2]\n",
    "        elif self.stop_loss == 3:\n",
    "            df_y = df_raw[['Y_3']].values\n",
    "            self.data_y = df_y[border1:border2]\n",
    "        elif self.stop_loss == 4:\n",
    "            df_y = df_raw[['Y_4']].values\n",
    "            self.data_y = df_y[border1:border2]\n",
    "        elif self.stop_loss == 5:\n",
    "            df_y = df_raw[['Y_5']].values\n",
    "            self.data_y = df_y[border1:border2]\n",
    "        else:\n",
    "            raise ValueError('You should choose stop_loss as 0, 2, 3, or 4.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        stock_x = self.data_x[index]\n",
    "        stock_y = self.data_y[index]\n",
    "        return torch.tensor(stock_x, dtype=torch.float32), torch.tensor(stock_y, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884257a6-15ff-43b7-9647-a8ed737a84cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 0] X shape: torch.Size([4, 437]), Y shape: torch.Size([4, 1])\n",
      "X sample:\n",
      "tensor([[ 0.8199,  0.8438,  0.1584,  ...,  0.7431,  1.1336,  0.6971],\n",
      "        [ 1.1693,  1.1908, -0.3769,  ...,  0.4974,  0.7799,  0.4695],\n",
      "        [-0.5096, -0.4766, -0.1894,  ..., -1.2263, -0.5057, -1.2610],\n",
      "        [-0.5003, -0.4574,  0.1706,  ...,  0.3923, -0.2825,  0.3405]])\n",
      "Y sample:\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]])\n",
      "[Batch 1] X shape: torch.Size([4, 437]), Y shape: torch.Size([4, 1])\n",
      "X sample:\n",
      "tensor([[-0.9709, -0.9480, -0.4256,  ..., -0.6532, -0.8453, -0.6706],\n",
      "        [ 0.3592,  0.3710,  0.3514,  ...,  0.6079,  0.4092,  0.5944],\n",
      "        [-0.3730, -0.3396,  0.4986,  ..., -0.2251, -0.0562, -0.2507],\n",
      "        [ 1.3208,  1.3511,  1.2623,  ...,  1.2508,  1.4033,  1.2846]])\n",
      "Y sample:\n",
      "tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=4, help='batch size')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "train_dataset = Dataset_SNP(\n",
    "        args=args, \n",
    "        root_path='./dataset/',      \n",
    "        flag='train', \n",
    "        data_path='SNP.csv', \n",
    "        scale=True, \n",
    "        stop_loss=0\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    print(f\"[Batch {i}] X shape: {batch_x.shape}, Y shape: {batch_y.shape}\")\n",
    "    print(f\"X sample:\\n{batch_x}\\nY sample:\\n{batch_y}\")\n",
    "    if i == 1:  \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64974acc-f418-4b33-983e-947ae695edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"./dataset/SNP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1793d02d-5a75-4d50-bad9-a308f2572b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 437])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae320e4a-dcb1-4ed0-a88b-dd2b6973b3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
